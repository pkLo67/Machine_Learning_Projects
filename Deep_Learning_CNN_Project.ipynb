{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of images using convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The objective of this project:\n",
    "\n",
    "The goal of this project is to build and train convolutional neural networks for classification of images in the CIFAR-10 Dataset. CIFAR-10 is composed of 60000 32x32 color images in 10 classes, with 6000 images per class, which is a well-known dataset for image classification. This dataset has 50000 training images and 10000 test images.\n",
    "\n",
    "The 10 classes are:\n",
    "\n",
    "0. airplane\n",
    "1. automobile\n",
    "2. bird\n",
    "3. cat\n",
    "4. deer\n",
    "5. dog\n",
    "6. frog\n",
    "7. horse\n",
    "8. ship\n",
    "9. truck\n",
    "\n",
    "For details about CIFAR-10 see: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An outline of the Deep Learning project plan:\n",
    "\n",
    "1. Load the CIFAR-10 dataset and examine its data structure.\n",
    "2. Engineer the dataset to create the train and test datasets, and scale them for CNN modeling.\n",
    "3. Build the first CNN model and train it using the RMSprop optimizer. Assess the prediction accuracy of the trained CNN model.\n",
    "4. Train the CNN model with the adam optimizer and assess the its prediction accuracy.\n",
    "5. Build the second CNN model with a more complicated neural architecture and test its prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset can be imported from the Keras library using the load_data() method. The loaded CIFAR-10 dataset contain two tuples, which are training and test sets. There are x and y values for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "y_test shape: (10000, 1)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The CIFAR-10 dataset was loaded and split into train and test datasets. Their data structures were examined.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Examine the shape of each image. Each image is a 32 x 32 x 3 numpy\n",
    "x_train[253].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification of the image:  [3]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAaTklEQVR4nO2dbYwdZ3XH/+e+7dq768Rvu7FMghMTUqK0hGgVIaVCFFoUEFJAKgg+oHxIMaqIBBL9EKVSSdVWgqqA8gFRmSYlVJSQkiCiKmqJUqjFl4BJE8fBFIJxsbGxE7/urvfl3pnTD3dcbdL5n92de/deJ8//J1nenXOfmTPPnTNz9/nfc465O4QQr39qw3ZACDEYFOxCJIKCXYhEULALkQgKdiESQcEuRCI0ehlsZrcDuB9AHcA/uPvnotdPTIz59u1bSm3uOR/I5EELDhZIitGx8jzjtmyp3A2LHOG2PA9kz8gUntsgpVR+bvXmhvIRxp8v4TVQ0Q8jtnAOg8m3GvefHWulfXrOzjs4r1q57ezZ85ibu1hqrBzsZlYH8GUAfwTgGIAfm9nj7v5TNmb79i34m7/6VKmt3Vmgx8rzDvOBjvFskdqy4Fhzs6e57fyJ0u0bWsEFjBa1zc+V3zwAwPk9BwuLfFyet8kO63yHAVkWOFJrUtPWHTeVbrfGBB3Tac/zYwU3glrgR71WPv/tNr8+8uBYzdFR7kedh1Onw+dxcb78vBt1/p41W+XnfP+X/5GO6eVj/K0AXnT3w+6+BOBhAHf0sD8hxDrSS7DvBHB02e/Him1CiMuQXoK97DP0//vDxMz2mNl+M9s/c2G2h8MJIXqhl2A/BuDqZb+/AcDxV7/I3fe6+7S7T09sGu/hcEKIXugl2H8M4Hozu9bMWgA+AuDx/rglhOg3lVfj3b1jZncD+Hd0pbcH3f2FFQZRuckDGYrKJIHKlGV8RTVaYY4Wn0+fmSndvnGEj2m2yiUoAGgvBD52yhUIoJq8lueBPBjtLrDlVDICOp3y1e56bYwfKjqvilIku946wRvt4OdVD64rWLVrjvkfvi3MGAzqSWd39ycAPNHLPoQQg0HfoBMiERTsQiSCgl2IRFCwC5EICnYhEqGn1fh+EmYakYSXWIJa+/4AoNniiSsjG8pltMWF83RMa3QTtTlI0grijLgoyc6d3L+rJsMFB8uDbJ1OpzxZp9aKJK+AUJVbu2wbyYawatdVdDnmFeTSOJty7ejJLkQiKNiFSAQFuxCJoGAXIhEU7EIkwmWzGl+ljlseJCVEK/ULi7wslTtPQGmNlJcCatZ4qaV6g2fJ1Ov8XpsZ9yNet2aljKrVdwtrtQUr2hkpjxXVmYsTYaIahVFZsPJx0Wp8UCYvvE6jKzg6XqQm9BM92YVIBAW7EImgYBciERTsQiSCgl2IRFCwC5EIg5XeDHCmT4StnNgYLk91Ohep7czpY9Q2OsqlspGR8qSWvB5IedQCWK1am6E4p4IkmgTJHXHpt0iyCzNyyOZqdeaiLi3hCVi5BNhhnXMA1ConoATvWVS7js1xUNOuCnqyC5EICnYhEkHBLkQiKNiFSAQFuxCJoGAXIhF6kt7M7AiAGXT1no67T0evd4SV4agllF0ICwvlDe4BIEg2Qy2QmhrNclluIQ96RgVnXKvzc67VWfbaCu2OSF243Pn+srD1VvC+RO8ZaTcVZcpFddrCmnG1IION+BhJinlwzpHqGT05o/PmEluFOoSBatgPnf0P3P3lPuxHCLGO6GO8EInQa7A7gO+Z2U/MbE8/HBJCrA+9foy/zd2Pm9kkgCfN7Gfuvm/5C4qbwB4A2Lr1yh4PJ4SoSk9Pdnc/Xvx/CsB3ANxa8pq97j7t7tMTm3hvbiHE+lI52M1szMwmLv0M4D0ADvbLMSFEf+nlY/wUgO8UBfgaAP7Z3f+tL169Clbkz2pcTpqb4dJbq1ZeOBIAxjdspDYma9Vr5W2hAODCDM++q9dHqW3j2Dj3I5Qpy+WaLFAHs7BwZ1BUMouy9sovrSyQk7KoKGOgeTWCDMEatUUZe9wUE0iYLBsx9oSPIecV7atysLv7YQBvrTpeCDFYJL0JkQgKdiESQcEuRCIo2IVIBAW7EIlw2fR6q0KkkLQXudRx+vRv+cCpzdTUZEqZ8V5vmzdvp7b5uRlqq9d44cuOt6jNiAyYB1le7c4SP1aHF/WsdyKpjEhvQYZgkGwWpHnFWYDM/6Ulfs6NBpd0IxcXFrnc227zoqStVrkUHJ0X7zkXZFJSixDidYWCXYhEULALkQgKdiESQcEuRCJcNqvx0cpjRrM41l4vDgDOn+Or4FeM8ZVuVguvHSSETI4Hq/GLL1EbjN+HRzZMUlu9Va4MdNp8lTYLVnA7HrQ0CubfiFbSznjbpawd7C+okxetrDMajWqX/uzcHLVlGVcu4o5Sa19Z5/ES1DyMXBBCvH5QsAuRCAp2IRJBwS5EIijYhUgEBbsQiTBY6c2NZjtENcaYbhHJGSNjvC6ct7gs5y1eF67RLHdyaWmWjvnViy9QWyfj4xq0JRBgOfcfzfJEHjc+hrVIAqKEC8CDcRl5Q/N2kAgTyGtmPDklgtWg47Xpqp1Xd6dBAk3gf45yW5BnBCMJPsEU6skuRCoo2IVIBAW7EImgYBciERTsQiSCgl2IRFhRejOzBwG8H8Apd7+p2LYFwLcA7AJwBMCH3f3syodzmq0TSm/E5kGGz8gol5q2TPKsscYG3nzywsVzpdsXF3l9sdNneUZW5lyOGWvx+/DoWHCPXiw/XiT9RNlaedA3Kqonxzo5RdmNEVnQhiouXrf27DCPNN04fa3SuJz5H+lonfK5j+Z3NU/2rwG4/VXb7gHwlLtfD+Cp4nchxGXMisFe9Fs/86rNdwB4qPj5IQAf6LNfQog+U/Vv9il3PwEAxf/8c7EQ4rJg3RfozGyPme03s/0zM7zKhxBifaka7CfNbAcAFP+fYi90973uPu3u0xMTfPFLCLG+VA32xwHcWfx8J4Dv9scdIcR6sRrp7ZsA3glgm5kdA/BZAJ8D8IiZ3QXg1wA+tNoDssymSDKglkByiVr4bN8eFGys8/vfufPl0luzuYmO2blrC7Xldf5JZyTwo1nnb9tSVj7OmRaG6nJYNIxKrFV3GBS3jK6DShJg8Ai0IFsuyhCMMgujDDxGlfdsxWB3948S07vXfDQhxNDQN+iESAQFuxCJoGAXIhEU7EIkgoJdiEQYcMHJapIBG2IV71WtoA8calwiaY2Ol26PZLKxTVx6a4xyWxb0j1ucn6c21iIuzwLpqjKBXFrpfQ4KTgbSVdV9BqMCPziR9FbF/VDKq5B9pye7EImgYBciERTsQiSCgl2IRFCwC5EICnYhEmGg0puDF4msIrvkQUG+sPBe1OcryKAaJX3gojtmlKFWD/SYTiC95UGhytxZIUI6ZAV5KprjYBghD7LvIjwqONnnjLLoxKpmCPaftfuhJ7sQiaBgFyIRFOxCJIKCXYhEULALkQiDTYRBtNod1fZiK/i8/VC0QmssWwSABautIySBph0kmeTWpLZacM4etGSKYGpHHtRwi+rTzZyfobbWaLk6AQAZUUoiBSUiTEAJbGz1P1qlt4or/5GSE11zMHK8MNll7c9pPdmFSAQFuxCJoGAXIhEU7EIkgoJdiERQsAuRCKtp//QggPcDOOXuNxXb7gPwcQAvFS+7192fWM0BaSJEIFs4kY3Y9q6PwX0sOFZYf4zINdGxcvCklU6g8HQqJ6esXdpqLy5R2+FfHKa2a667ltpGN5A6f6FcynEPezKteX9RQk5cS67auOhtYXJkmLBFDxUkgHEX/o+vAbi9ZPuX3P3m4t+qAl0IMTxWDHZ33wfgzAB8EUKsI738zX63mR0wswfNbHPfPBJCrAtVg/0rAHYDuBnACQBfYC80sz1mtt/M9s/MzFU8nBCiVyoFu7ufdPfMu+VDvgrg1uC1e9192t2nJyZ4P3IhxPpSKdjNbMeyXz8I4GB/3BFCrBerkd6+CeCdALaZ2TEAnwXwTjO7Gd11/iMAPrGqo7kDOcnmCmSLnOgWYV21qEZXqLytvfVPox5ktgVZb1mnzd3IuQ1B5pgRW5SRderky9T26yO/obaLi9zHN+7aWbp9bGwjHRMmeRmXMC2oycfkMCajdvcXZK8Ftqr19ZjEFsmoVSTWFYPd3T9asvmBNR9JCDFU9A06IRJBwS5EIijYhUgEBbsQiaBgFyIRBlxw0lEjxfzagZSQkUwpi7K/KraGChU7kl0VSW+NeovalmZPUtvceZ6OMLZpO7XlVv6Wzpw/T8ccOniI2hYu8oy4o786Tm3nzp4r3b5z51V0zDVvLJfrAKDZ4pcqk2aBQIKNJNYgw66qHNZvGa0KerILkQgKdiESQcEuRCIo2IVIBAW7EImgYBciEQbe6y0nWW+sNxgAZKSwZI30gAN6kTP4uEazPLuqTnrAAUA9kOXq7Vlqmz/Ds81GWjxzzFubyvc3d5GOmbnA/Yj89yAT7eLMYun2My9zCXBsY5ARV+fZZq1RPv9jY+U1FFotLomGGZMBUcHJ0MbyKfusyOnJLkQiKNiFSAQFuxCJoGAXIhEU7EIkwkBX491zLC0tEE9G6bg66xhUi3rqRJ4EbXqCOmj1WvkKbqs1HuyPT/Fih9TjAzC/xFfPcZ7XjJu4snweW03ux+hGPvee8dX4+SXu/+hI+T5feqk8QQYAjh77LbV1Mn6ssXG+iv/mG3aXbt+9m7euagYr/9HTsRZcV6GNmNj2lfzo5xghxGsQBbsQiaBgFyIRFOxCJIKCXYhEULALkQiraf90NYCvA7gKXUFrr7vfb2ZbAHwLwC50W0B92N3PRvvKsxwX58qTLsauCOQfVkYs4/panAfD73G1ILmj3ihPuKgHsmGgTmHRg2SM5hXUNjI2wceR5KBGi5/z1FW8pt35c/PUtpgzTRS0l9PF+fIEGQBYXOL7W1jgtfDm5ngbqvn58vp6FkiiN/3um6mt0eDXB2vj1LWtfVyN1DwEgFqdjakmG16iA+Az7v4WAG8H8EkzuxHAPQCecvfrATxV/C6EuExZMdjd/YS7P1P8PAPgEICdAO4A8FDxsocAfGC9nBRC9M6a/mY3s10A3gbgaQBT7n4C6N4QAEz22zkhRP9YdbCb2TiARwF82t0vrGHcHjPbb2b7Z+f4339CiPVlVcFuZk10A/0b7v5Ysfmkme0o7DsAnCob6+573X3a3afHxzb0w2chRAVWDHbr1tN5AMAhd//iMtPjAO4sfr4TwHf7754Qol+sJuvtNgAfA/C8mT1bbLsXwOcAPGJmdwH4NYAPrbSjLMswR9oQjU1soeOYipZlXKrxIO2tFqUTYe3SW63GM8MyUnOvu0MuoY2MTVFbXuNSX4ecN802BDAeZI0tLXENcy6Qw86dK/9Lb6nN58MD6coC6Spibrb8vA8+/zM6ZsuW8jp+APCm66+htnojknR5qBmRe1m7MQCoE+nNgmt7xWB39x+C54S+e6XxQojLA32DTohEULALkQgKdiESQcEuRCIo2IVIhIEWnMzzDi5eKJfeogw2JrC1g4KNgYoTErU7qjHJq8anMXeekVVv8S8ZNUd51lsnmCsjRTjngvZPp06+RG2zs3yOz5+bobaL80TqC96YdjvIogsIM72IZDdzgX+b85lnDlDb9kn+vmyf5PKxBc9V1hoqOK0V5GMyZs0jhBCvSRTsQiSCgl2IRFCwC5EICnYhEkHBLkQiDFZ6y3LMzZRnQy0u8Kwsb5bLJ1kgQUX93CKaQU80lt2WB250gqKMUWZenvNssyjbj/UUawSZcrOzvAjkubNcolpaiiTA8ufIUiCvReccJb1FMitTqJh/AHD2DJcUT548TW2Tk7xwZ6XnangJhxVV++WBEOK1iIJdiERQsAuRCAp2IRJBwS5EIgx0Nb7ZGsVV19xYahsZ4QkGzQ3ltd/ao+N0TJxEwO9xzSg5pTlWuj0Plk1Hg9XnbAP3vx20Qlpa5LXfGo1yX7ZN8pp2kzt2UNvCEk+S8TpP8rm4WJ5448F8RO9L1M+rHi3Vs9X44PqI1I5jR39LbTfcwNtGtVrR6jlJhAmX45UII4QgKNiFSAQFuxCJoGAXIhEU7EIkgoJdiERYUXozs6sBfB3AVQByAHvd/X4zuw/AxwFc0mbudfcnon2NjG7ErhtuKbUFygqtt5VbkIgRaSsBUZshIy18GkE9sM0NnoByxaYrqW1qKkoYiRKAyieyHszHrl2/Q20zM9USYdqdcv+XlrhsGElvL/7859T2n9//D2pzL/cjCxKUoivnxPHS/qUAeMsrAJiaitqblb9na091iVmNzt4B8Bl3f8bMJgD8xMyeLGxfcve/67NPQoh1YDW93k4AOFH8PGNmhwDsXG/HhBD9ZU1/s5vZLgBvA/B0seluMztgZg+a2eY++yaE6COrDnYzGwfwKIBPu/sFAF8BsBvAzeg++b9Axu0xs/1mtv/CzGwfXBZCVGFVwW5mTXQD/Rvu/hgAuPtJd8/cPQfwVQC3lo11973uPu3u05sm+HfBhRDry4rBbt1l7QcAHHL3Ly7bvjx74oMADvbfPSFEv1jNavxtAD4G4Hkze7bYdi+Aj5rZzegqBEcAfGLFPdXqaIxuKjV5oL0x1agejuECSijLVVDsQokkuJ02LGgJVPErEEzGsWCuNpYn8wEAtk1yP2qB/0B5vT7mHwCMj/NPfpPbtlHbvh9w6a1DJMCofZI7P6+5OS4dnjl9ltp27OD16VgtwshHq9D+aTWr8T9EeQiEmroQ4vJC36ATIhEU7EIkgoJdiERQsAuRCAp2IRJhoAUna1ZDq0kKOgZyGJMgLJB+IlkolOUiSYPtMlTygmMhyrCL7sMV9MEgQ7D7vShm49lhkR9m5efWaXfomKUO92N+gbeoitpGsfc6av+EnL8vUSHQA8/9jNqiVmVbt20t3T46Ul5oFQAajfLQzYjUCOjJLkQyKNiFSAQFuxCJoGAXIhEU7EIkgoJdiEQYqPQGB0CKJcYFIsvvSUFyEskjKvYWHMojWcsr9OQKM+wiebC/0qEH9/Xwjh9m5gXDWI+1oKBnHmTmtQPJLirAybIpo35u0f6icYd/eZTajh49QW1jJO2w1WrRMUx6O3uWF73Uk12IRFCwC5EICnYhEkHBLkQiKNiFSAQFuxCJMFDpLfcci+2LpTYPM5fK70lW4+5H2Wv1Or/HNRrlhRK748qPF8lJ0f3Ua/yc81rQxy7Yp+VkrugIwCN5MJDDokKbNbLPPMj+iopRNkf4e739Kl7MkfWWa7fbdExki3rVdYKsPQ8yHGdn58qP1T5Hx7BwWQp815NdiERQsAuRCAp2IRJBwS5EIijYhUiEFVfjzWwUwD4AI8Xrv+3unzWzawE8DGALgGcAfMzd+VIlAMCDWlzB2i4xmQcrjyRpBQDyoKxap81PoUbqltUCVaDR4MkM9RZf+a+RGm5dKqyeR8kzwZFWaG4VjCKr8YHqwtogAcB1b9pNbX+y5+PU1umUJ9Cw7UC84h7Z2kt8n1ECTZYRHwPlYpEkBj327UfpmNU82RcBvMvd34pue+bbzeztAD4P4Evufj2AswDuWsW+hBBDYsVg9y6XGqs3i38O4F0Avl1sfwjAB9bFQyFEX1htf/Z60cH1FIAnAfwSwDl3v/RZ4hiAnevjohCiH6wq2N09c/ebAbwBwK0A3lL2srKxZrbHzPab2f4LF2aqeyqE6Ik1rca7+zkAPwDwdgBXmtmllak3ADhOxux192l3n960aaIXX4UQPbBisJvZdjO7svh5A4A/BHAIwPcB/HHxsjsBfHe9nBRC9M5qEmF2AHjIuv18agAecfd/NbOfAnjYzP4awH8BeGDlXdkKkhIZRbShqINPrV6ttVIkQ7EaaVFrnyzjbYsskPmaI1yyazVHqY2dQJTsEta0C2yRKGekpVQe6Z7B5I+0eCukqakpamNyadUWYPVAZkVwbbNaeAAwtmm8dPvmrTzBJyO72/eDfXTMisHu7gcAvK1k+2F0/34XQrwG0DfohEgEBbsQiaBgFyIRFOxCJIKCXYhEsEgS6PvBzF4C8D/Fr9sAvDywg3PkxyuRH6/ktebHG929VLMbaLC/4sBm+919eigHlx/yI0E/9DFeiERQsAuRCMMM9r1DPPZy5McrkR+v5HXjx9D+ZhdCDBZ9jBciEYYS7GZ2u5n9t5m9aGb3DMOHwo8jZva8mT1rZvsHeNwHzeyUmR1ctm2LmT1pZr8o/t88JD/uM7PfFHPyrJm9bwB+XG1m3zezQ2b2gpl9qtg+0DkJ/BjonJjZqJn9yMyeK/z4y2L7tWb2dDEf3zIznhpZhrsP9B+AOrplra4D0ALwHIAbB+1H4csRANuGcNx3ALgFwMFl2/4WwD3Fz/cA+PyQ/LgPwJ8NeD52ALil+HkCwM8B3DjoOQn8GOicoJvsO1783ATwNLoFYx4B8JFi+98D+NO17HcYT/ZbAbzo7oe9W3r6YQB3DMGPoeHu+wCcedXmO9At3AkMqIAn8WPguPsJd3+m+HkG3eIoOzHgOQn8GCjepe9FXocR7DsBHF32+zCLVTqA75nZT8xsz5B8uMSUu58AuhcdgMkh+nK3mR0oPuav+58TyzGzXejWT3gaQ5yTV/kBDHhO1qPI6zCCvawMyLAkgdvc/RYA7wXwSTN7x5D8uJz4CoDd6PYIOAHgC4M6sJmNA3gUwKfd/cKgjrsKPwY+J95DkVfGMIL9GICrl/1Oi1WuN+5+vPj/FIDvYLiVd06a2Q4AKP4/NQwn3P1kcaHlAL6KAc2JmTXRDbBvuPtjxeaBz0mZH8Oak+LYay7yyhhGsP8YwPXFymILwEcAPD5oJ8xszMwmLv0M4D0ADsaj1pXH0S3cCQyxgOel4Cr4IAYwJ9YtCPcAgEPu/sVlpoHOCfNj0HOybkVeB7XC+KrVxvehu9L5SwB/PiQfrkNXCXgOwAuD9APAN9H9ONhG95POXQC2AngKwC+K/7cMyY9/AvA8gAPoBtuOAfjx++h+JD0A4Nni3/sGPSeBHwOdEwC/h24R1wPo3lj+Ytk1+yMALwL4FwAja9mvvkEnRCLoG3RCJIKCXYhEULALkQgKdiESQcEuRCIo2IVIBAW7EImgYBciEf4X/+15yx1m+ZAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualization of an image dataset.\n",
    "\n",
    "print('The classification of the image: ', y_train[253])\n",
    "plt.imshow(x_train[253]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use keras.utils.to_categorical() to convert an integer between 0-9 to a vector with a 1 in the (Pythonic) 9th position\n",
    "# This data conversion is required for training CNN models\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the converted dataset\n",
    "y_train[253]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data type into float and scale the data values\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a CNN deep learning model:\n",
    "\n",
    "#### Keras Layers for CNNs:\n",
    "\n",
    "Neural Networks are primarily composed of the Dense, Activation and Dropout Layers. For building a CNN model, there are the CNN-specific layers provided by Keras. These CNN-specific layers are listed below. The Keras Sequential API is used to build neural layers of the CNN model.\n",
    "\n",
    "#### Conv2D:\n",
    "\n",
    "The convolutional (Conv2D) layer is the first one to add into CNN. It contains filters to transform the image data using the kernel filter matrix.\n",
    "\n",
    "To execute the Conv2D function to build a CNN model, the following Keras function is used.\n",
    "\n",
    "**keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, \\**kwargs)**\n",
    "\n",
    "\n",
    "**A few parameters explained:**\n",
    "\n",
    "* filters: the number of filter used per location. In other words, the depth of the output.\n",
    "* kernel_size: an (x,y) tuple giving the height and width of the kernel to be used\n",
    "* strides: and (x,y) tuple giving the stride in each dimension. Default is (1,1)\n",
    "* input_shape: required only for the first layer\n",
    "\n",
    "Note: the size of the output will be determined by the kernel_size, strides\n",
    "\n",
    "\n",
    "#### MaxPooling2D:\n",
    "\n",
    "MaxPooling2D is the second important neural layer in CNN. It functions as a downsampling filter to extract more important feature data values for reducing the computational cost and overfitting. By combining both Conv2D and MaxPooling2D, CNN can integrate local features and identify the global features of the image.\n",
    "\n",
    "**keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)**\n",
    "\n",
    "* pool_size: the (x,y) size of the grid to be pooled.\n",
    "* strides: Assumed to be the pool_size unless otherwise specified\n",
    "\n",
    "\n",
    "#### Dropout:\n",
    "\n",
    "Dropout is a method for the regularization of CNN. Setting Dropout in CNN can randomly ignore a proportion of neural nodes (adjusting their weights to zero) for each training step. Setting Dropout drives CNN to learn features in a distributed manner. The inclusion of this method in CNN improves generalization and minimizes the overfitting.\n",
    "\n",
    "\n",
    "#### Flatten:\n",
    "\n",
    "This layer is to turn the input into a one-dimensional vector (per instance). This layer is required when transitioning between convolutional layers and fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 32)        2432      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 6, 6, 32)          25632     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               147968    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 181,162\n",
      "Trainable params: 181,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the first CNN model using the Keras Sequential API\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "\n",
    "## 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## 2x2 max pooling reduces to 3 x 3 x 32\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "## Flatten turns 3x3x32 into 288x1\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(512))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes))\n",
    "model_1.add(Activation('softmax'))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "50000/50000 [==============================] - 22s 430us/step - loss: 1.7329 - accuracy: 0.3673 - val_loss: 1.5380 - val_accuracy: 0.4570\n",
      "Epoch 2/15\n",
      "50000/50000 [==============================] - 21s 424us/step - loss: 1.4512 - accuracy: 0.4763 - val_loss: 1.3203 - val_accuracy: 0.5358\n",
      "Epoch 3/15\n",
      "50000/50000 [==============================] - 21s 411us/step - loss: 1.3444 - accuracy: 0.5218 - val_loss: 1.2338 - val_accuracy: 0.5560\n",
      "Epoch 4/15\n",
      "50000/50000 [==============================] - 21s 419us/step - loss: 1.2726 - accuracy: 0.5481 - val_loss: 1.1623 - val_accuracy: 0.5884\n",
      "Epoch 5/15\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 1.2212 - accuracy: 0.5662 - val_loss: 1.1993 - val_accuracy: 0.5733\n",
      "Epoch 6/15\n",
      "50000/50000 [==============================] - 24s 485us/step - loss: 1.1882 - accuracy: 0.5808 - val_loss: 1.0991 - val_accuracy: 0.6059\n",
      "Epoch 7/15\n",
      "50000/50000 [==============================] - 20s 395us/step - loss: 1.1573 - accuracy: 0.5926 - val_loss: 1.1267 - val_accuracy: 0.6063\n",
      "Epoch 8/15\n",
      "50000/50000 [==============================] - 21s 417us/step - loss: 1.1327 - accuracy: 0.6018 - val_loss: 1.0718 - val_accuracy: 0.6251\n",
      "Epoch 9/15\n",
      "50000/50000 [==============================] - 20s 396us/step - loss: 1.1177 - accuracy: 0.6076 - val_loss: 1.1204 - val_accuracy: 0.6144\n",
      "Epoch 10/15\n",
      "50000/50000 [==============================] - 20s 406us/step - loss: 1.1043 - accuracy: 0.6146 - val_loss: 1.1122 - val_accuracy: 0.6173\n",
      "Epoch 11/15\n",
      "50000/50000 [==============================] - 22s 438us/step - loss: 1.0922 - accuracy: 0.6197 - val_loss: 1.0109 - val_accuracy: 0.6416\n",
      "Epoch 12/15\n",
      "50000/50000 [==============================] - 29s 581us/step - loss: 1.0806 - accuracy: 0.6230 - val_loss: 1.0731 - val_accuracy: 0.6324\n",
      "Epoch 13/15\n",
      "50000/50000 [==============================] - 20s 401us/step - loss: 1.0782 - accuracy: 0.6261 - val_loss: 1.0338 - val_accuracy: 0.6415\n",
      "Epoch 14/15\n",
      "50000/50000 [==============================] - 20s 404us/step - loss: 1.0732 - accuracy: 0.6311 - val_loss: 1.1021 - val_accuracy: 0.6292\n",
      "Epoch 15/15\n",
      "50000/50000 [==============================] - 22s 433us/step - loss: 1.0744 - accuracy: 0.6312 - val_loss: 1.0609 - val_accuracy: 0.6412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x19427fc8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0005, decay=1e-6)\n",
    "\n",
    "# Train the model using RMSprop\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=15,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.32166566e-02, 9.79335629e-04, 1.42665163e-01, ...,\n",
       "        2.66466420e-02, 1.62161347e-02, 1.89722108e-03],\n",
       "       [9.58724245e-02, 4.11439687e-01, 3.97793974e-06, ...,\n",
       "        1.28629525e-08, 4.89496440e-01, 3.18735279e-03],\n",
       "       [1.52348578e-01, 2.55635902e-02, 1.32777197e-02, ...,\n",
       "        4.14361758e-03, 7.69058287e-01, 1.63525622e-02],\n",
       "       ...,\n",
       "       [1.08572957e-03, 2.46698473e-05, 8.75498056e-02, ...,\n",
       "        3.10110636e-02, 4.75588429e-04, 1.52245397e-04],\n",
       "       [2.16618031e-02, 6.47548353e-03, 1.74624205e-01, ...,\n",
       "        8.98089707e-02, 3.00113112e-04, 2.22325631e-04],\n",
       "       [1.75226171e-06, 5.82319304e-09, 2.28551755e-04, ...,\n",
       "        9.69778001e-01, 1.74387371e-09, 1.01512931e-07]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The predictions of x_test from the CNN model are probabilities for classes\n",
    "model_1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, ..., 5, 4, 7], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use predict_classes to identify which class has the highest probability for each prediction\n",
    "model_1.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, ..., 5, 1, 7], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6412"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The prediction accuracy of the first CNN model using the RMSprop optimizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(np.argmax(y_test, axis=1), model_1.predict_classes(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CNN model using adam as the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "50000/50000 [==============================] - 22s 443us/step - loss: 1.6895 - accuracy: 0.3792 - val_loss: 1.4237 - val_accuracy: 0.4862\n",
      "Epoch 2/15\n",
      "50000/50000 [==============================] - 20s 397us/step - loss: 1.4299 - accuracy: 0.4789 - val_loss: 1.2836 - val_accuracy: 0.5360\n",
      "Epoch 3/15\n",
      "50000/50000 [==============================] - 20s 400us/step - loss: 1.3243 - accuracy: 0.5255 - val_loss: 1.2174 - val_accuracy: 0.5753\n",
      "Epoch 4/15\n",
      "50000/50000 [==============================] - 22s 445us/step - loss: 1.2577 - accuracy: 0.5513 - val_loss: 1.1790 - val_accuracy: 0.5858\n",
      "Epoch 5/15\n",
      "50000/50000 [==============================] - 20s 408us/step - loss: 1.2046 - accuracy: 0.5713 - val_loss: 1.1186 - val_accuracy: 0.6134\n",
      "Epoch 6/15\n",
      "50000/50000 [==============================] - 21s 428us/step - loss: 1.1609 - accuracy: 0.5863 - val_loss: 1.0684 - val_accuracy: 0.6245\n",
      "Epoch 7/15\n",
      "50000/50000 [==============================] - 23s 453us/step - loss: 1.1271 - accuracy: 0.6010 - val_loss: 1.0465 - val_accuracy: 0.6369\n",
      "Epoch 8/15\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 1.0980 - accuracy: 0.6117 - val_loss: 1.0485 - val_accuracy: 0.6334\n",
      "Epoch 9/15\n",
      "50000/50000 [==============================] - 21s 422us/step - loss: 1.0681 - accuracy: 0.6205 - val_loss: 1.0035 - val_accuracy: 0.6503\n",
      "Epoch 10/15\n",
      "50000/50000 [==============================] - 21s 425us/step - loss: 1.0482 - accuracy: 0.6297 - val_loss: 1.0164 - val_accuracy: 0.6424\n",
      "Epoch 11/15\n",
      "50000/50000 [==============================] - 22s 440us/step - loss: 1.0217 - accuracy: 0.6390 - val_loss: 0.9689 - val_accuracy: 0.6653\n",
      "Epoch 12/15\n",
      "50000/50000 [==============================] - 21s 412us/step - loss: 1.0034 - accuracy: 0.6447 - val_loss: 0.9577 - val_accuracy: 0.6649\n",
      "Epoch 13/15\n",
      "50000/50000 [==============================] - 22s 441us/step - loss: 0.9816 - accuracy: 0.6548 - val_loss: 0.9683 - val_accuracy: 0.6654\n",
      "Epoch 14/15\n",
      "50000/50000 [==============================] - 23s 453us/step - loss: 0.9728 - accuracy: 0.6552 - val_loss: 0.9413 - val_accuracy: 0.6740\n",
      "Epoch 15/15\n",
      "50000/50000 [==============================] - 22s 450us/step - loss: 0.9548 - accuracy: 0.6645 - val_loss: 0.9322 - val_accuracy: 0.6736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2126d388>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the first CNN model using the adam optimizer\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "\n",
    "## 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## 2x2 max pooling reduces to 3 x 3 x 32\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "## Flatten turns 3x3x32 into 288x1\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(512))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes))\n",
    "model_1.add(Activation('softmax'))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# initiate adam optimizer\n",
    "opt = keras.optimizers.adam(lr=0.0005)\n",
    "\n",
    "# Train the model using the adam optimizer\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=15,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6736"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The prediction accuracy of the first CNN model using the adam optimizer\n",
    "accuracy_score(np.argmax(y_test, axis=1), model_1.predict_classes(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on accuracy score analysis, adam is better than RMSprop as the optimizer for training the CNN model for classification prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the second CNN model with a more complicated neural architecture:\n",
    "\n",
    "Build a more complicated model with the following pattern:\n",
    "Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "* Set kernel size as 3 X 3.\n",
    "* Use strides of 1 for all convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the second CNN model\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first Conv2D layer with 32 filters\n",
    "model_2.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "# Add the second Conv2D layer with 32 filters\n",
    "model_2.add(Conv2D(32, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "# Add the first MaxPooling2D layer\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "# Add the third Conv2D layer with 64 filters\n",
    "model_2.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "# Add the fourth Conv2D layer with 64 filters\n",
    "model_2.add(Conv2D(64, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "# Add the second MaxPooling2D layer\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "# Add Flatten\n",
    "model_2.add(Flatten())\n",
    "\n",
    "# Add the Dense layer\n",
    "model_2.add(Dense(512))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "\n",
    "# Add the final output layer\n",
    "model_2.add(Dense(num_classes))\n",
    "model_2.add(Activation('softmax'))\n",
    "\n",
    "# Check number of parameters\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 177s 4ms/step - loss: 1.5761 - accuracy: 0.4230 - val_loss: 1.2350 - val_accuracy: 0.5499\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 180s 4ms/step - loss: 1.1589 - accuracy: 0.5885 - val_loss: 0.9742 - val_accuracy: 0.6575\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 175s 3ms/step - loss: 0.9700 - accuracy: 0.6595 - val_loss: 0.8450 - val_accuracy: 0.7005\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 171s 3ms/step - loss: 0.8579 - accuracy: 0.7001 - val_loss: 0.8221 - val_accuracy: 0.7107\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 168s 3ms/step - loss: 0.7886 - accuracy: 0.7229 - val_loss: 0.7507 - val_accuracy: 0.7368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x237c1808>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiate adam optimizer\n",
    "opt_2 = keras.optimizers.adam(lr=0.0005)\n",
    "\n",
    "# Train the second CNN model using adam\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt_2,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_2.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=5,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7368"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The prediction accuracy of the second CNN model using the adam optimizer\n",
    "accuracy_score(np.argmax(y_test, axis=1), model_2.predict_classes(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "This CNN project for image classification shows that the CNN model with a more complicated neural architecture has a better prediction performance when it is trained by the adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
